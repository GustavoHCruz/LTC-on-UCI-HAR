{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchmetrics\n",
    "from ncps.torch import LTC\n",
    "from ncps.wirings import AutoNCP\n",
    "from plyer import notification\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "sys.path.append(os.path.abspath(\"funcs\"))\n",
    "\n",
    "from config_reading import read_configs\n",
    "from timer_callback import TimingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = read_configs()\n",
    "processing_configs = configs[\"processing\"]\n",
    "training_configs = configs[\"training\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"c_{configs[\"model_name\"]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the CUDA float32 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(training_configs[\"float_precision\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed manually to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1234"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = training_configs[\"seed\"]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "pl.seed_everything(seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_validation = processing_configs[\"validation_proportion\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_train_x.pt\"))\n",
    "train_y = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_train_y.pt\"))\n",
    "if has_validation:\n",
    "\tval_x = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_val_x.pt\"))\n",
    "\tval_y = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_val_y.pt\"))\n",
    "test_x = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_test_x.pt\"))\n",
    "test_y = torch.load(os.path.join(processing_configs[\"save_path\"], \"tensor_test_y.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loaders, models phases and model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch, val_batch, test_batch = training_configs[\"batch_sizes\"]\n",
    "\n",
    "if train_batch == \"all\":\n",
    "  train_batch = train_x.shape[0]\n",
    "if has_validation and val_batch == \"all\":\n",
    "  val_batch = val_x.shape[0]\n",
    "if test_batch == \"all\":\n",
    "  test_batch = test_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(data.TensorDataset(train_x, train_y), shuffle=True, num_workers=16, persistent_workers=True, batch_size=train_batch)\n",
    "if has_validation:\n",
    "\tval_dataloader = data.DataLoader(data.TensorDataset(val_x, val_y), num_workers=16, persistent_workers=True, batch_size=val_batch)\n",
    "test_dataloader = data.DataLoader(data.TensorDataset(test_x, test_y), num_workers=16, persistent_workers=True, batch_size=test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceLearner(pl.LightningModule):\n",
    "\tdef __init__(self, model, lr):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = model\n",
    "\t\tself.lr = lr\n",
    "\t\tself.loss_fn = nn.CrossEntropyLoss()\n",
    "\t\tself.acc_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=6)\n",
    "\n",
    "\tdef training_step(self, batch):\n",
    "\t\tx, y = batch\n",
    "\t\ty_hat, _ = self.model.forward(x)\n",
    "\t\tloss = self.loss_fn(y_hat, y)\n",
    "\t\ty_pred = y_hat.argmax(dim=-1)\n",
    "\t\tself.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "\t\tself.log(\"train_acc\", self.acc_fn(y_pred, y), on_step=False, on_epoch=True)\n",
    "\t\treturn {\"loss\": loss}\n",
    "\n",
    "\tdef validation_step(self, batch):\n",
    "\t\tx, y = batch\n",
    "\t\ty_hat, _ = self.model.forward(x)\n",
    "\t\tloss = self.loss_fn(y_hat, y)\n",
    "\t\ty_pred = y_hat.argmax(dim=-1)\n",
    "\t\tself.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "\t\tself.log(\"val_acc\", self.acc_fn(y_pred, y), on_step=False, on_epoch=True)\n",
    "\t\treturn {\"loss\": loss}\n",
    "\t\n",
    "\tdef test_step(self, batch):\n",
    "\t\tx, y = batch\n",
    "\t\ty_hat, _ = self.model.forward(x)\n",
    "\t\tloss = self.loss_fn(y_hat, y)\n",
    "\t\ty_pred = y_hat.argmax(dim=-1)\n",
    "\t\tself.log(\"test_loss\", loss)\n",
    "\t\tself.log(\"test_acc\", self.acc_fn(y_pred, y))\n",
    "\t\treturn {\"loss\": loss}\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn torch.optim.Adam(self.model.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_features = 6 # Output\n",
    "in_features = 561 # Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiring = AutoNCP(training_configs[\"num_neurons\"], out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alloc!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ltc_model = LTC(in_features, wiring, batch_first=True)\n",
    "learn = SequenceLearner(ltc_model, lr=training_configs[\"learning_rate\"])\n",
    "\n",
    "log_dir = f\"logs\"\n",
    "logger = CSVLogger(log_dir, name=model_name)\n",
    "\n",
    "# checkpoint_dir = f\"{log_dir}/{model_name}/checkpoints\"\n",
    "# last_checkpoint_path = f\"{checkpoint_dir}/last.ckpt\"\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "# \tdirpath=checkpoint_dir,\n",
    "# \tfilename=\"{epoch}-{val_loss:.2f}\",\n",
    "# \tsave_top_k=1,\n",
    "# \tmonitor=\"val_loss\",\n",
    "# \tmode=\"min\",\n",
    "# \tsave_last=True\n",
    "# )\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "\tlogger=logger,\n",
    "\tmax_epochs=training_configs[\"max_epochs\"],\n",
    "  check_val_every_n_epoch=1,\n",
    "  callbacks=[TimingCallback()],\n",
    "\tgradient_clip_val=1  # Clip gradient to stabilize training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: logs\\c_ex003\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | model   | LTC                | 442 K \n",
      "1 | loss_fn | CrossEntropyLoss   | 0     \n",
      "2 | acc_fn  | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------\n",
      "354 K     Trainable params\n",
      "88.2 K    Non-trainable params\n",
      "442 K     Total params\n",
      "1.770     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gusta\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=0]        "
     ]
    }
   ],
   "source": [
    "if has_validation:\n",
    "\ttrainer.fit(learn, train_dataloader, val_dataloader)\n",
    "else:\n",
    "\ttrainer.fit(learn, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(learn, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"models\"):\n",
    "\tos.makedirs(\"models\")\n",
    "\n",
    "torch.save(ltc_model, f\"models/{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the configuration used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = os.path.join(logger.log_dir, \"config.json\")\n",
    "with open(config_file_path, 'w') as config_file:\n",
    "\tjson.dump(configs, config_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notification for finishing the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notification.notify(\n",
    "\ttitle=\"Training ended\",\n",
    "\tmessage=f\"The training of the model {model_name} with {training_configs[\"max_epochs\"]} epochs has been completed.\",\n",
    "\ttimeout=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
